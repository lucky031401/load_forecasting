# -*- coding: utf-8 -*-
"""predCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ORrixCN37nQATteCAAr8CFoggNSGZEcu
"""

import pandas as pd
import numpy as np

# 設定輸入資料、輸出資料的長度
input_length = 144
output_length = 144

x = pd.read_csv( '/home/g22qkqkq/load-forecast/data/sample/sampleInputData_2.csv', index_col=0 )
x_load = np.array(x.loc[ :, 'load' ])
x_bodyComfort = np.array(x.loc[ :, 'bodyComfort' ])
x_lastWeek = np.array(x.loc[ :, 'lastWeek' ])


x_load = np.reshape( x_load, (len(x_load), 1) )
x_bodyComfort = np.reshape( x_bodyComfort, (len(x_bodyComfort), 1) )
x_lastWeek = np.reshape( x_lastWeek, (len(x_lastWeek), 1) )

# normalization
from sklearn.preprocessing import MinMaxScaler
# 需要兩個轉換模型 因為單位不同
scaler_load = MinMaxScaler(feature_range=(0,1))
scaler_bodyComfort = MinMaxScaler(feature_range=(0,1))
x_load = scaler_load.fit_transform(x_load)
x_lastWeek = scaler_load.transform(x_lastWeek)
x_bodyComfort = scaler_bodyComfort.fit_transform(x_bodyComfort)

x_input = np.concatenate( (x_load,x_bodyComfort), axis=1 )
x_input = np.concatenate( (x_input,x_lastWeek), axis=1 )

x_input = np.reshape(x_input,(1,x_input.shape[0],x_input.shape[1]))

print( x_input.shape )

from keras.models import Sequential
from keras import backend as K
from keras.layers import Dense, Activation, LSTM, Flatten, RepeatVector, Permute, Multiply, Lambda, merge
from keras import optimizers, Input, Model

def model_lstm_selfAttention(
    input_shape,
    output_length,
):
    inputs = Input(shape=input_shape)
    output_lstm1 = LSTM(288,return_sequences=True,input_shape=input_shape) (inputs)
    output_lstm2 = LSTM(144,return_sequences=False) (output_lstm1)

    e = Dense(1, activation='tanh') (output_lstm2)
    # Now do all the softmax business taking the above o/p
    e = Flatten() (e)
    a = Activation('softmax') (e)
    temp = RepeatVector(144) (a)
    temp = Permute([2, 1]) (temp)
    # multiply weight with lstm layer o/p
    output = merge.Multiply()([output_lstm2, temp])
    # Get the attention adjusted output state
    output = Lambda(lambda values: K.sum(values, axis=1))(output)
    return Model(inputs, output)


model = model_lstm_selfAttention(
    input_shape = (x_input.shape[1],x_input.shape[2]),
    output_length = output_length
)

model.compile(optimizer='adam',loss='mse',metrics=['mae'])

# 輸出模型摘要資訊
model.summary()

# load model param
model.load_weights('/home/g22qkqkq/load-forecast/data/sample/29-0.0006-0.0032.hdf5')



# 以模型預估
predResult = model.predict( x_input )

# 轉回原數值(actual value)
predResult = scaler_load.inverse_transform(predResult)

# to one dim
predResult = np.reshape( predResult,(predResult.shape[1]) )

# save as npy file

pd.DataFrame(predResult).to_csv("/home/g22qkqkq/load-forecast/data/sample/sample_2.csv",header=None,index=None)

print("forkin"+"me")





